{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "import nltk.data\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import pymysql\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.twitter.com/oauth/authenticate?oauth_token=w3sNGQAAAAAAzGLWAAABW-toN34'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from newspaper import Article\n",
    "from twython import Twython\n",
    "##set up access to twitter website\n",
    "consumer_key = 'BE7MnHVqk6bgXqdKMb0jGIbun'\n",
    "consumer_secret = 'bWnvrCaAfrQlAgLM2gGgUwGvLefOsl41zMQf1kpUfvPTBXd41O'\n",
    "access_token = '714916782142840834-I1zEtpMksHaqbqQYPynOPQxHbCNlyAJ'\n",
    "access_secret = '8yqnE4DkVJ96Qm8OohyaszNcYj5JQEffB43ESIaeBcDj0'\n",
    " \n",
    "#auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "#auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "twitter=Twython(consumer_key,consumer_secret)\n",
    "auth=twitter.get_authentication_tokens()\n",
    "OAUTH_TOKEN=auth['oauth_token']\n",
    "OAUTH_TOKEN_SECRET=auth['oauth_token_secret']\n",
    "auth['auth_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#api = tweepy.API(auth)\n",
    "\n",
    "#function definitions\n",
    "def clean_tweet(tweet):\n",
    "        pattern = re.compile('[^A-Za-z0-9]+')\n",
    "        tweet = pattern.sub(' ', tweet)\n",
    "        #print encoded_tweet\n",
    "       # et = pattern.sub(' ', tweet)\n",
    "    #print encoded_tweet\n",
    "\n",
    "        words = tweet.split()\n",
    "\n",
    "    # Filter unnecessary words\n",
    "        for w in words:\n",
    "            if w.startswith(\"RT\") or w.startswith(\"www\") or w.startswith(\"http\") or w.startswith(\"https\"):\n",
    "                words.remove(w)\n",
    "        \n",
    "    #code = str(words.encode('utf-8'))\n",
    "   #     print(words)\n",
    "        return str(words)\n",
    "    \n",
    "def get_tweet_sentiment(tweet):\n",
    "        \n",
    "        analysis = TextBlob(clean_tweet(tweet))\n",
    "        if analysis.sentiment.polarity > 0.5:\n",
    "            return 'excited'\n",
    "        elif analysis.sentiment.polarity <= 0.5 and analysis.sentiment.polarity > 0 :\n",
    "            return 'happy'\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 'neutral'\n",
    "        \n",
    "        elif analysis.sentiment.polarity > -0.5 and analysis.sentiment.polarity < 0:\n",
    "            return 'sad'\n",
    "        \n",
    "        else:\n",
    "            return 'morose'\n",
    "tweets=[]\n",
    "def get_tweets(query, count=50):\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            fetched_tweets =api.search(q=query, count=count)\n",
    "            for tweet in fetched_tweets:\n",
    "                parsed_tweet={}\n",
    "                parsed_tweet['text']=tweet.text\n",
    "                parsed_tweet['sentiment']=get_tweet_sentiment(tweet.text)\n",
    "            \n",
    "                if tweet.retweet_count > 0:\n",
    "                    if parsed_tweet not in tweets:\n",
    "                        tweets.append(parsed_tweet)\n",
    "                else:\n",
    "                    tweets.append(parsed_tweet)\n",
    "            return tweets\n",
    "        except tweepy.TweepError as e:\n",
    "             print(\"Error : \"+str(e))\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "        sentences=self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences=[self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "    \n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pos_tag(self, sentences):\n",
    "        pos=[nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        pos=[[(word,word,[postag]) for (word, postag) in sentence]for sentence in pos]\n",
    "        return pos\n",
    "    \n",
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    if isinstance(key,bool):\n",
    "                        if key:\n",
    "                            key = \"True\"\n",
    "                        else:\n",
    "                            key = \"False\"\n",
    "                    self.max_key_size = max(self.max_key_size,len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicttagger = DictionaryTagger([ '/home/vaidehi/Documents/positive.yml', '/home/vaidehi/Documents/negative.yml', '/home/vaidehi/Documents/inc.yml', '/home/vaidehi/Documents/dec.yml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "def value_of(sentiment):\n",
    "    if sentiment=='positive':\n",
    "        return 1.0\n",
    "    if sentiment =='negative':\n",
    "        return -1.0\n",
    "    return 0.0\n",
    "    \n",
    "    \n",
    "def sentiment_score(review):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in review])\n",
    "\n",
    "def normalization(score):\n",
    "    min_a=-20.0\n",
    "    max_a=+20.0\n",
    "    a=-1.0\n",
    "    b=1.0\n",
    "    x_norm= a + (score-min_a)*(b-a)/(max_a-min_a)\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "world_news=[]\n",
    "\n",
    "f = urlopen('http://timesofindia.indiatimes.com/world')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.w_tle'):\n",
    "    a=\"http://timesofindia.indiatimes.com/world\"\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'http://timesofindia.indiatimes.com/world' in s:\n",
    "        world_news.append(link.find('a').attrs['href'])\n",
    "    elif 'photogallery' in s or 'video' in s or 'photostory' in s:\n",
    "        continue\n",
    "    else:\n",
    "        world_news.append(a+s)\n",
    "f = urlopen('http://indianexpress.com/section/world/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.short-info'):\n",
    "      world_news.append(link.find('a').attrs['href'])\n",
    "        \n",
    "for link in soup.select('.story'):\n",
    "     world_news.append(link.find('a').attrs['href'])\n",
    "        \n",
    "for link in soup.select('.title'):\n",
    "     world_news.append(link.find('a').attrs['href'])\n",
    "f = urlopen('http://www.hindustantimes.com/world-news/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.media-body'):\n",
    "    world_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.headingfive'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        world_news.append(link.find('a').attrs['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(world_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "national_news=[]\n",
    "\n",
    "f = urlopen('http://indianexpress.com/section/india/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.short-info'):\n",
    "    national_news.append(link.find('a').attrs['href'])\n",
    "        \n",
    "for link in soup.select('.story'):\n",
    "    national_news.append(link.find('a').attrs['href'])\n",
    "        \n",
    "for link in soup.select('.title'):\n",
    "    national_news.append(link.find('a').attrs['href'])\n",
    "\n",
    "f = urlopen('http://www.hindustantimes.com/india-news/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.top-news-sec'):\n",
    "        national_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.headingfive'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        national_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.media-body'):\n",
    "    national_news.append(link.find('a').attrs['href'])\n",
    "f = urlopen('http://timesofindia.indiatimes.com/india')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.clearFix'):\n",
    "    a=\"http://timesofindia.indiatimes.com\"\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'http://timesofindia.indiatimes.com' in s:\n",
    "        national_news.append(link.find('a').attrs['href'])\n",
    "    elif 'photogallery' in s or 'video' in s or 'photostory' in s:\n",
    "        continue\n",
    "    else:\n",
    "        national_news.append(a+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entertainment_news=[]\n",
    "f = urlopen('http://www.hindustantimes.com/entertainment/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.media-body'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        entertainment_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.random-heading'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        entertainment_news.append(link.find('a').attrs['href'])\n",
    "len(entertainment_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(national_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifestyle_news=[]\n",
    "f = urlopen('http://www.hindustantimes.com/lifestyle/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.media-body'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        lifestyle_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.headingfive'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        lifestyle_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.random-heading'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        lifestyle_news.append(link.find('a').attrs['href'])\n",
    "len(lifestyle_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Indian Express Technology\n",
    "tech_news=[]\n",
    "f = urlopen('http://indianexpress.com/section/technology/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.review-list'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        tech_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('h3'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        tech_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.common-link'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        tech_news.append(link.find('a').attrs['href'])\n",
    "        \n",
    "len(tech_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_news=[]\n",
    "f = urlopen('http://timesofindia.indiatimes.com/sports')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.list2'):\n",
    "    a=\"http://timesofindia.indiatimes.com\"\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'http://' in s:\n",
    "        sports_news.append(link.find('a').attrs['href'])\n",
    "    elif 'photogallery' in s or 'video' in s or 'photostory' in s:\n",
    "        continue\n",
    "    else:\n",
    "        sports_news.append(a+s)\n",
    "for link in soup.select('.main-story'):\n",
    "    a=\"http://timesofindia.indiatimes.com\"\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'http://' in s:\n",
    "        sports_news.append(link.find('a').attrs['href'])\n",
    "    elif 'photogallery' in s or 'video' in s or 'photostory' in s:\n",
    "        continue\n",
    "    else:\n",
    "        sports_news.append(a+s)\n",
    "f = urlopen('http://www.hindustantimes.com/sports-news/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.media-body'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        sports_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.headingfive'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        sports_news.append(link.find('a').attrs['href'])\n",
    "len(sports_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_news=[]\n",
    "f = urlopen('http://indianexpress.com/business/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.short-info'):\n",
    "    business_news.append(link.find('a').attrs['href'])\n",
    "        \n",
    "for link in soup.select('.story'):\n",
    "    business_news.append(link.find('a').attrs['href'])\n",
    "        \n",
    "for link in soup.select('.title'):\n",
    "    business_news.append(link.find('a').attrs['href'])\n",
    "f = urlopen('http://timesofindia.indiatimes.com/business')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.w_tle'):\n",
    "    a=\"http://timesofindia.indiatimes.com\"\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'http://' in s:\n",
    "        business_news.append(link.find('a').attrs['href'])\n",
    "    elif 'photogallery' in s or 'video' in s or 'photostory' in s:\n",
    "        continue\n",
    "    else:\n",
    "        business_news.append(a+s)\n",
    "\n",
    "f = urlopen('http://www.hindustantimes.com/business-news/')\n",
    "soup = BeautifulSoup(f.fp,\"lxml\")\n",
    "for link in soup.select('.media-body'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        business_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.random-heading'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        business_news.append(link.find('a').attrs['href'])\n",
    "for link in soup.select('.headingfive'):\n",
    "    s=link.find('a').attrs['href']\n",
    "    if 'photos' in s or 'video' in s:\n",
    "        continue\n",
    "    else:\n",
    "        business_news.append(link.find('a').attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(business_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656\n"
     ]
    }
   ],
   "source": [
    "db=pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "cursor = db.cursor()\n",
    "pic=cursor.execute('SELECT max(article_id) FROM articles ')\n",
    "pic=cursor.fetchall()\n",
    "lastnum=pic[0][0]\n",
    "print(lastnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8500000000000001\n",
      "query commited\n",
      "0.25\n",
      "query commited\n",
      "0.30000000000000004\n",
      "query commited\n",
      "0.7\n",
      "query commited\n",
      "0.75\n",
      "query commited\n",
      "0.19999999999999996\n",
      "query commited\n",
      "-0.25\n",
      "query commited\n",
      "0.19999999999999996\n",
      "query commited\n",
      "-0.09999999999999998\n",
      "query commited\n",
      "1.0499999999999998\n",
      "query commited\n",
      "-0.050000000000000044\n",
      "query commited\n",
      "0.050000000000000044\n",
      "query commited\n",
      "-0.19999999999999996\n",
      "query commited\n",
      "0.0\n",
      "query commited\n",
      "0.75\n",
      "query commited\n",
      "0.3500000000000001\n",
      "query commited\n",
      "0.19999999999999996\n",
      "query commited\n",
      "0.1499999999999999\n",
      "query commited\n",
      "-0.44999999999999996\n",
      "query commited\n",
      "0.6499999999999999\n",
      "query commited\n",
      "0.050000000000000044\n",
      "query commited\n",
      "-0.55\n"
     ]
    },
    {
     "ename": "TwythonError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[1;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;31m# sending a valid response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[0;32m    267\u001b[0m                                      \" response\")\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m                 )\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    648\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[1;32m--> 649\u001b[1;33m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[0;32m    650\u001b[0m             \u001b[0mretries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[1;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;31m# sending a valid response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[0;32m    267\u001b[0m                                      \" response\")\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/twython/api.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, url, method, params, api_call)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mrequests_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTwythonError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-636f7a1030c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mentities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0mfetched_tweets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'statuses'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetched_tweets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/twython/endpoints.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'search/tweets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'id'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'statuses'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/twython/api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, endpoint, params, version)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1.1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;34m\"\"\"Shortcut for GET requests via :class:`request`\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1.1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/twython/api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, endpoint, method, params, version)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         content = self._request(url, method=method, params=params,\n\u001b[1;32m--> 258\u001b[1;33m                                 api_call=url)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vaidehi/anaconda3/lib/python3.5/site-packages/twython/api.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, url, method, params, api_call)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mrequests_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTwythonError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# create stash for last function intel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTwythonError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))"
     ]
    }
   ],
   "source": [
    "\n",
    "if lastnum == None:\n",
    "    pic=1\n",
    "else:\n",
    "    pic=lastnum+1\n",
    "\n",
    "for url in business_news:\n",
    "      \n",
    "    article=Article(url)\n",
    "    article.download()\n",
    "    try:\n",
    "        article.parse()\n",
    "    except newspaper.ArticleException:\n",
    "        continue\n",
    "\n",
    "    content = article.text\n",
    "    splitter= Splitter()\n",
    "    postagger = POSTagger()\n",
    "    splitted_sentences=splitter.split(content)\n",
    "    #print(splitted_sentences)\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #print(dict_tagged_sentences)\n",
    "    norm_arti=normalization(sentiment_score(dict_tagged_sentences))\n",
    "\n",
    "    print(norm_arti)\n",
    "    \n",
    "    #summary generation for each article\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    toz=sent_detector.tokenize(content.strip())\n",
    "    bow_matrix = CountVectorizer(ngram_range=(1,3)).fit_transform(toz)\n",
    "    normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    "    bow_matrix_1 = CountVectorizer(stop_words={'english'},ngram_range=(2,3)).fit_transform(toz)\n",
    "    normalized_1 = TfidfTransformer().fit_transform(bow_matrix_1)\n",
    "    similarity_graph_1 = normalized_1 * normalized_1.T #sentence\n",
    "    similarity_graph = normalized * normalized.T #sentence\n",
    "    similarity_graph.toarray()\n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    nx_graph_1 = nx.from_scipy_sparse_matrix(similarity_graph_1)\n",
    "    scores_1 = nx.pagerank(nx_graph_1)\n",
    "    sorted_list= sorted(([scores[i],s] for i,s in enumerate(toz[0:len(toz)])),\n",
    "              reverse=True)\n",
    "    summary=  [item[1] for item in sorted_list][0:3]\n",
    "    s_x=''.join(summary)\n",
    "\n",
    "    s_x\n",
    "    \n",
    "    #store the article headline\n",
    "    y=article.title\n",
    "    #download the main image of the article\n",
    "    path=\"/home/vaidehi/FlaskApp/static/images/\"\n",
    "    \n",
    "    \n",
    "    w=article.top_image\n",
    "    if w =='':\n",
    "        storepath=\"p1.jpg\"\n",
    "    \n",
    "    #try:\n",
    "    else:\n",
    "        \n",
    "        urllib.request.urlretrieve(w, path+\"i\"+str(pic)+\".jpg\")\n",
    "        pic=pic+1\n",
    "        storepath=\"i\"+str(pic)+\".jpg\" \n",
    "\n",
    "    #except urllib.URLError:\n",
    "        #\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #print(y)\n",
    "    #print(w)\n",
    "    \n",
    "    #determine the sentiments of the twitterverse on the news\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    #tagged_sentences\n",
    "    \n",
    "    entity_names = []\n",
    "    for tree in chunked_sentences:\n",
    "    # Print results per sentence\n",
    "    # print extract_entity_names(tree)\n",
    "        entity_names.extend(extract_entity_names(tree))\n",
    "    entity=set(entity_names)\n",
    "    entities=[]\n",
    "    for word in entity:\n",
    "        entities.append(str(word))\n",
    "    for entity in entities:\n",
    "        results =twitter.search(q=entity, count=50)\n",
    "        fetched_tweets=results['statuses']\n",
    "        for tweet in fetched_tweets:\n",
    "            parsed_tweet={}\n",
    "            parsed_tweet['text']=tweet['text']\n",
    "    #print(parsed_tweet['text'])\n",
    "            parsed_tweet['sentiment']=get_tweet_sentiment(tweet['text'])\n",
    "    #print(parsed_tweet['sentiment'])\n",
    "            if tweet['retweet_count'] > 0:\n",
    "               if parsed_tweet not in tweets:\n",
    "                    tweets.append(parsed_tweet)\n",
    "            else:\n",
    "                tweets.append(parsed_tweet)\n",
    "    \n",
    "    try:\n",
    "        etweets=[tweet for tweet in tweets if tweet['sentiment']== 'excited'] \n",
    "        #print(\"happy tweets percentage: {} %\".format(100*len(htweets)/len(tweets)))\n",
    "\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        print(\"No happy tweets\")\n",
    "    et=int(100*len(etweets)/len(tweets))\n",
    "    \n",
    "    try:\n",
    "        htweets=[tweet for tweet in tweets if tweet['sentiment']== 'happy']\n",
    "        #print(\"amused tweets percentage: {} %\".format(100*len(atweets)/len(tweets)))\n",
    "\n",
    "\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        print(\"No amused tweets\")\n",
    "    ht=int(100*len(htweets)/len(tweets))\n",
    "\n",
    "    try:    \n",
    "        ntweets=[tweet for tweet in tweets if tweet['sentiment']== 'neutral']\n",
    "        #print(\"neutral tweets percentage: {} %\".format(100*len(ntweets)/len(tweets)))\n",
    "        ht=int(100*len(htweets)/len(tweets))\n",
    "\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        print(\"No neutral tweets\")\n",
    "    nt=int(100*len(ntweets)/len(tweets))\n",
    "\n",
    "    try:    \n",
    "        stweets=[tweet for tweet in tweets if tweet['sentiment']== 'sad']\n",
    "        #print(\"sad tweets percentage: {} %\".format(100*len(stweets)/len(tweets)))\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "         print(\"No sad tweets\")\n",
    "    st=int(100*len(stweets)/len(tweets))\n",
    "    \n",
    "    try:\n",
    "        mtweets=[tweet for tweet in tweets if tweet['sentiment']== 'morose']    \n",
    "        #print(\"morose tweets percentage: {} %\".format(100*len(mtweets)/len(tweets)))\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        print(\"No morose tweets\") \n",
    "    mt=int(100*len(mtweets)/len(tweets))\n",
    "    genre_id=3\n",
    "    db = pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "    cursor = db.cursor()\n",
    "    sql = \"INSERT INTO `articles` (`headline`,`content`,`sentiment_score`,`summary`,`genre_id`,`image_url`,`elated`,`happy`,`neutral`,`sad`,`morose`) VALUES (%s,%s, %s, %s, %s,%s,%s, %s, %s, %s,%s)\"\n",
    "    cursor.execute(sql,(y,content,norm_arti,s_x,genre_id,storepath,et,ht,nt,st,mt))\n",
    "    print('query commited')\n",
    "    db.commit()\n",
    "\n",
    "\n",
    "    db.close()\n",
    "# disconnect from server\n",
    "\n",
    "    \n",
    "#urls_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clustering of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import spectral_clustering\n",
    "import pymysql\n",
    "tagged_pos_doc_list=[]\n",
    "concat_tagged_pos_doc_list=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaidehi/anaconda3/lib/python3.5/site-packages/sklearn/manifold/spectral_embedding_.py:217: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n"
     ]
    }
   ],
   "source": [
    "db = pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "cursor = db.cursor()\n",
    "sql = \"SELECT * from articles where genre_id=2\"\n",
    "cursor.execute(sql)\n",
    "articles=cursor.fetchall()\n",
    "news=[]\n",
    "tagged_pos_doc_list=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for article in articles:\n",
    "    article_dict={'ArticleId': article[0],'Headline':article[1], 'Date':article[3],'Summary':article[7],'Image':article[4] ,'GenreId': article[5],'Content':article[2],'Excited':article[8],'Happy':article[9],'Neutral':article[10],'Sad':article[11],'Morose':article[12]}\n",
    "    news.append(article_dict)\n",
    "for i in range(len(news)):\n",
    "    tagged_sent = pos_tag(news[i]['Content'].split())\n",
    "    tagged_sent_proper_nouns=[]\n",
    "    for x in range(len(tagged_sent)):\n",
    "        if tagged_sent[x][1]=='NNP':\n",
    "            tagged_sent_proper_nouns.append(tagged_sent[x][0])\n",
    "        tagged_dict={'tag':news[i]['ArticleId'],'propernoun':tagged_sent_proper_nouns}\n",
    "    tagged_pos_doc_list.append(tagged_dict)\n",
    "y=len(tagged_pos_doc_list)\n",
    "data_set=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for items in tagged_pos_doc_list:\n",
    "    ax=' '.join(items['propernoun'])\n",
    "    str_list=[]\n",
    "    str_list.append(ax)\n",
    "    concat_dict={'tag':items['tag'],'concatstr':str_list}\n",
    "    concat_tagged_pos_doc_list.append(concat_dict)\n",
    "concatstr_list=[]\n",
    "for items in concat_tagged_pos_doc_list:\n",
    "    concatstr_list.append(items['concatstr'][0])\n",
    "\n",
    "data_set=''.join(concatstr_list)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(concatstr_list)\n",
    "X_train_counts.toarray()\n",
    "similarity_graph = X_train_counts * X_train_counts.T #sentence\n",
    "similarity_graph.toarray()\n",
    "z=(int)(y *0.3)\n",
    "spectral = cluster.SpectralClustering(n_clusters=z,eigen_solver='arpack',affinity=\"precomputed\")\n",
    "spectral.fit(similarity_graph)\n",
    "clusters = spectral.fit_predict(similarity_graph)\n",
    "clusters_list=[]\n",
    "for i in range(z):\n",
    "    cluster_no=[]\n",
    "    for j in range(len(clusters)):\n",
    "        if clusters[j]==i:\n",
    "            cluster_no.append(concat_tagged_pos_doc_list[j]['tag'])\n",
    "    clusters_list.append(cluster_no)\n",
    "import csv\n",
    "\n",
    "with open(\"/home/vaidehi/FlaskApp/cluster/world.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "cursor = db.cursor()\n",
    "sql = \"SELECT * from articles where genre_id=1\"\n",
    "cursor.execute(sql)\n",
    "articles=cursor.fetchall()\n",
    "news=[]\n",
    "tagged_pos_doc_list=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for article in articles:\n",
    "    article_dict={'ArticleId': article[0],'Headline':article[1], 'Date':article[3],'Summary':article[7],'Image':article[4] ,'GenreId': article[5],'Content':article[2],'Excited':article[8],'Happy':article[9],'Neutral':article[10],'Sad':article[11],'Morose':article[12]}\n",
    "    news.append(article_dict)\n",
    "for i in range(len(news)):\n",
    "    tagged_sent = pos_tag(news[i]['Content'].split())\n",
    "    tagged_sent_proper_nouns=[]\n",
    "    for x in range(len(tagged_sent)):\n",
    "        if tagged_sent[x][1]=='NNP':\n",
    "            tagged_sent_proper_nouns.append(tagged_sent[x][0])\n",
    "        tagged_dict={'tag':news[i]['ArticleId'],'propernoun':tagged_sent_proper_nouns}\n",
    "    tagged_pos_doc_list.append(tagged_dict)\n",
    "y=len(tagged_pos_doc_list)\n",
    "data_set=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for items in tagged_pos_doc_list:\n",
    "    ax=' '.join(items['propernoun'])\n",
    "    str_list=[]\n",
    "    str_list.append(ax)\n",
    "    concat_dict={'tag':items['tag'],'concatstr':str_list}\n",
    "    concat_tagged_pos_doc_list.append(concat_dict)\n",
    "concatstr_list=[]\n",
    "for items in concat_tagged_pos_doc_list:\n",
    "    concatstr_list.append(items['concatstr'][0])\n",
    "\n",
    "data_set=''.join(concatstr_list)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(concatstr_list)\n",
    "X_train_counts.toarray()\n",
    "similarity_graph = X_train_counts * X_train_counts.T #sentence\n",
    "similarity_graph.toarray()\n",
    "z=(int)(y *0.3)\n",
    "spectral = cluster.SpectralClustering(n_clusters=z,eigen_solver='arpack',affinity=\"precomputed\")\n",
    "spectral.fit(similarity_graph)\n",
    "clusters = spectral.fit_predict(similarity_graph)\n",
    "clusters_list=[]\n",
    "for i in range(z):\n",
    "    cluster_no=[]\n",
    "    for j in range(len(clusters)):\n",
    "        if clusters[j]==i:\n",
    "            cluster_no.append(concat_tagged_pos_doc_list[j]['tag'])\n",
    "    clusters_list.append(cluster_no)\n",
    "import csv\n",
    "\n",
    "with open(\"/home/vaidehi/FlaskApp/cluster/national.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(clusters_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"/home/vaidehi/FlaskApp/cluster/national.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(clusters_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "cursor = db.cursor()\n",
    "sql = \"SELECT * from articles where genre_id=3\"\n",
    "cursor.execute(sql)\n",
    "articles=cursor.fetchall()\n",
    "news=[]\n",
    "tagged_pos_doc_list=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for article in articles:\n",
    "    article_dict={'ArticleId': article[0],'Headline':article[1], 'Date':article[3],'Summary':article[7],'Image':article[4] ,'GenreId': article[5],'Content':article[2],'Excited':article[8],'Happy':article[9],'Neutral':article[10],'Sad':article[11],'Morose':article[12]}\n",
    "    news.append(article_dict)\n",
    "for i in range(len(news)):\n",
    "    tagged_sent = pos_tag(news[i]['Content'].split())\n",
    "    tagged_sent_proper_nouns=[]\n",
    "    for x in range(len(tagged_sent)):\n",
    "        if tagged_sent[x][1]=='NNP':\n",
    "            tagged_sent_proper_nouns.append(tagged_sent[x][0])\n",
    "        tagged_dict={'tag':news[i]['ArticleId'],'propernoun':tagged_sent_proper_nouns}\n",
    "    tagged_pos_doc_list.append(tagged_dict)\n",
    "y=len(tagged_pos_doc_list)\n",
    "data_set=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for items in tagged_pos_doc_list:\n",
    "    ax=' '.join(items['propernoun'])\n",
    "    str_list=[]\n",
    "    str_list.append(ax)\n",
    "    concat_dict={'tag':items['tag'],'concatstr':str_list}\n",
    "    concat_tagged_pos_doc_list.append(concat_dict)\n",
    "concatstr_list=[]\n",
    "for items in concat_tagged_pos_doc_list:\n",
    "    concatstr_list.append(items['concatstr'][0])\n",
    "\n",
    "data_set=''.join(concatstr_list)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(concatstr_list)\n",
    "X_train_counts.toarray()\n",
    "similarity_graph = X_train_counts * X_train_counts.T #sentence\n",
    "similarity_graph.toarray()\n",
    "z=(int)(y *0.3)\n",
    "spectral = cluster.SpectralClustering(n_clusters=z,eigen_solver='arpack',affinity=\"precomputed\")\n",
    "spectral.fit(similarity_graph)\n",
    "clusters = spectral.fit_predict(similarity_graph)\n",
    "clusters_list=[]\n",
    "for i in range(z):\n",
    "    cluster_no=[]\n",
    "    for j in range(len(clusters)):\n",
    "        if clusters[j]==i:\n",
    "            cluster_no.append(concat_tagged_pos_doc_list[j]['tag'])\n",
    "    clusters_list.append(cluster_no)\n",
    "import csv\n",
    "\n",
    "with open(\"/home/vaidehi/FlaskApp/cluster/business.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "cursor = db.cursor()\n",
    "sql = \"SELECT * from articles where genre_id=4\"\n",
    "cursor.execute(sql)\n",
    "articles=cursor.fetchall()\n",
    "news=[]\n",
    "tagged_pos_doc_list=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for article in articles:\n",
    "    article_dict={'ArticleId': article[0],'Headline':article[1], 'Date':article[3],'Summary':article[7],'Image':article[4] ,'GenreId': article[5],'Content':article[2],'Excited':article[8],'Happy':article[9],'Neutral':article[10],'Sad':article[11],'Morose':article[12]}\n",
    "    news.append(article_dict)\n",
    "for i in range(len(news)):\n",
    "    tagged_sent = pos_tag(news[i]['Content'].split())\n",
    "    tagged_sent_proper_nouns=[]\n",
    "    for x in range(len(tagged_sent)):\n",
    "        if tagged_sent[x][1]=='NNP':\n",
    "            tagged_sent_proper_nouns.append(tagged_sent[x][0])\n",
    "        tagged_dict={'tag':news[i]['ArticleId'],'propernoun':tagged_sent_proper_nouns}\n",
    "    tagged_pos_doc_list.append(tagged_dict)\n",
    "y=len(tagged_pos_doc_list)\n",
    "data_set=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for items in tagged_pos_doc_list:\n",
    "    ax=' '.join(items['propernoun'])\n",
    "    str_list=[]\n",
    "    str_list.append(ax)\n",
    "    concat_dict={'tag':items['tag'],'concatstr':str_list}\n",
    "    concat_tagged_pos_doc_list.append(concat_dict)\n",
    "concatstr_list=[]\n",
    "for items in concat_tagged_pos_doc_list:\n",
    "    concatstr_list.append(items['concatstr'][0])\n",
    "\n",
    "data_set=''.join(concatstr_list)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(concatstr_list)\n",
    "X_train_counts.toarray()\n",
    "similarity_graph = X_train_counts * X_train_counts.T #sentence\n",
    "similarity_graph.toarray()\n",
    "z=(int)(y *0.2)\n",
    "spectral = cluster.SpectralClustering(n_clusters=z,eigen_solver='arpack',affinity=\"precomputed\")\n",
    "spectral.fit(similarity_graph)\n",
    "clusters = spectral.fit_predict(similarity_graph)\n",
    "clusters_list=[]\n",
    "for i in range(z):\n",
    "    cluster_no=[]\n",
    "    for j in range(len(clusters)):\n",
    "        if clusters[j]==i:\n",
    "            cluster_no.append(concat_tagged_pos_doc_list[j]['tag'])\n",
    "    clusters_list.append(cluster_no)\n",
    "import csv\n",
    "\n",
    "with open(\"/home/vaidehi/FlaskApp/cluster/technology.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propernoun': ['Bombay',\n",
       "  'Stock',\n",
       "  'Dalal',\n",
       "  'Street.',\n",
       "  '(Express',\n",
       "  'Archive',\n",
       "  'Photo)',\n",
       "  'Bombay',\n",
       "  'Stock',\n",
       "  'Dalal',\n",
       "  'Street.',\n",
       "  '(Express',\n",
       "  'Archive',\n",
       "  'Photo)',\n",
       "  'Sensex',\n",
       "  'Nifty',\n",
       "  'BSE',\n",
       "  'Friday',\n",
       "  'Nifty',\n",
       "  'RBI',\n",
       "  'Vinod',\n",
       "  'Nair,',\n",
       "  'Head',\n",
       "  'Research,',\n",
       "  'Geojit',\n",
       "  'Financial',\n",
       "  'Services.',\n",
       "  'Wall',\n",
       "  'Street',\n",
       "  'US',\n",
       "  'Emmanuel',\n",
       "  'Macron’s',\n",
       "  'Banking',\n",
       "  'RBI',\n",
       "  'ICICI',\n",
       "  'Bank,',\n",
       "  'Axis',\n",
       "  'Bank,',\n",
       "  'SBI,',\n",
       "  'HDFC',\n",
       "  'Bank',\n",
       "  'Shares',\n",
       "  'Ambuja',\n",
       "  'Cements',\n",
       "  'ACC',\n",
       "  'Lupin',\n",
       "  'Bharti',\n",
       "  'Airtel',\n",
       "  '(1.91',\n",
       "  'Paints,',\n",
       "  'ONGC,',\n",
       "  'Infosys,',\n",
       "  'Sun',\n",
       "  'Pharma,',\n",
       "  'TCS,',\n",
       "  'Hero',\n",
       "  'MotoCorp',\n",
       "  'Sensex',\n",
       "  'BSE',\n",
       "  'IT,',\n",
       "  'Broader',\n",
       "  'However,',\n",
       "  'Rs',\n",
       "  'Globally,',\n",
       "  'Macron',\n",
       "  'Key',\n",
       "  'Japan,',\n",
       "  'Hong',\n",
       "  'Kong,',\n",
       "  'South',\n",
       "  'Korea',\n",
       "  'Taiwan',\n",
       "  'India',\n",
       "  'News,',\n",
       "  'Indian',\n",
       "  'Express',\n",
       "  'App'],\n",
       " 'tag': 287}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=len(tagged_pos_doc_list)\n",
    "tagged_pos_doc_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "cursor = db.cursor()\n",
    "sql = \"SELECT * from articles where genre_id=5\"\n",
    "cursor.execute(sql)\n",
    "articles=cursor.fetchall()\n",
    "news=[]\n",
    "tagged_pos_doc_list=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for article in articles:\n",
    "    article_dict={'ArticleId': article[0],'Headline':article[1], 'Date':article[3],'Summary':article[7],'Image':article[4] ,'GenreId': article[5],'Content':article[2],'Excited':article[8],'Happy':article[9],'Neutral':article[10],'Sad':article[11],'Morose':article[12]}\n",
    "    news.append(article_dict)\n",
    "for i in range(len(news)):\n",
    "    tagged_sent = pos_tag(news[i]['Content'].split())\n",
    "    tagged_sent_proper_nouns=[]\n",
    "    for x in range(len(tagged_sent)):\n",
    "        if tagged_sent[x][1]=='NNP':\n",
    "            tagged_sent_proper_nouns.append(tagged_sent[x][0])\n",
    "        tagged_dict={'tag':news[i]['ArticleId'],'propernoun':tagged_sent_proper_nouns}\n",
    "    tagged_pos_doc_list.append(tagged_dict)\n",
    "y=len(tagged_pos_doc_list)\n",
    "data_set=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for items in tagged_pos_doc_list:\n",
    "    ax=' '.join(items['propernoun'])\n",
    "    str_list=[]\n",
    "    str_list.append(ax)\n",
    "    concat_dict={'tag':items['tag'],'concatstr':str_list}\n",
    "    concat_tagged_pos_doc_list.append(concat_dict)\n",
    "concatstr_list=[]\n",
    "for items in concat_tagged_pos_doc_list:\n",
    "    concatstr_list.append(items['concatstr'][0])\n",
    "\n",
    "data_set=''.join(concatstr_list)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(concatstr_list)\n",
    "X_train_counts.toarray()\n",
    "similarity_graph = X_train_counts * X_train_counts.T #sentence\n",
    "similarity_graph.toarray()\n",
    "z=(int)(y *0.3)\n",
    "spectral = cluster.SpectralClustering(n_clusters=z,eigen_solver='arpack',affinity=\"precomputed\")\n",
    "spectral.fit(similarity_graph)\n",
    "clusters = spectral.fit_predict(similarity_graph)\n",
    "clusters_list=[]\n",
    "for i in range(z):\n",
    "    cluster_no=[]\n",
    "    for j in range(len(clusters)):\n",
    "        if clusters[j]==i:\n",
    "            cluster_no.append(concat_tagged_pos_doc_list[j]['tag'])\n",
    "    clusters_list.append(cluster_no)\n",
    "import csv\n",
    "\n",
    "with open(\"/home/vaidehi/FlaskApp/cluster/lifestyle.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "cursor = db.cursor()\n",
    "sql = \"SELECT * from articles where genre_id=6\"\n",
    "cursor.execute(sql)\n",
    "articles=cursor.fetchall()\n",
    "news=[]\n",
    "for article in articles:\n",
    "    article_dict={'ArticleId': article[0],'Headline':article[1], 'Date':article[3],'Summary':article[7],'Image':article[4] ,'GenreId': article[5],'Content':article[2],'Excited':article[8],'Happy':article[9],'Neutral':article[10],'Sad':article[11],'Morose':article[12]}\n",
    "    news.append(article_dict)\n",
    "for i in range(len(news)):\n",
    "    tagged_sent = pos_tag(news[i]['Content'].split())\n",
    "    tagged_sent_proper_nouns=[]\n",
    "    for x in range(len(tagged_sent)):\n",
    "        if tagged_sent[x][1]=='NNP':\n",
    "            tagged_sent_proper_nouns.append(tagged_sent[x][0])\n",
    "        tagged_dict={'tag':news[i]['ArticleId'],'propernoun':tagged_sent_proper_nouns}\n",
    "    tagged_pos_doc_list.append(tagged_dict)\n",
    "y=len(tagged_pos_doc_list)\n",
    "data_set=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for items in tagged_pos_doc_list:\n",
    "    ax=' '.join(items['propernoun'])\n",
    "    str_list=[]\n",
    "    str_list.append(ax)\n",
    "    concat_dict={'tag':items['tag'],'concatstr':str_list}\n",
    "    concat_tagged_pos_doc_list.append(concat_dict)\n",
    "concatstr_list=[]\n",
    "for items in concat_tagged_pos_doc_list:\n",
    "    concatstr_list.append(items['concatstr'][0])\n",
    "\n",
    "data_set=''.join(concatstr_list)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(concatstr_list)\n",
    "X_train_counts.toarray()\n",
    "similarity_graph = X_train_counts * X_train_counts.T #sentence\n",
    "similarity_graph.toarray()\n",
    "z=(int)(y *0.3)\n",
    "spectral = cluster.SpectralClustering(n_clusters=z,eigen_solver='arpack',affinity=\"precomputed\")\n",
    "spectral.fit(similarity_graph)\n",
    "clusters = spectral.fit_predict(similarity_graph)\n",
    "clusters_list=[]\n",
    "for i in range(z):\n",
    "    cluster_no=[]\n",
    "    for j in range(len(clusters)):\n",
    "        if clusters[j]==i:\n",
    "            cluster_no.append(concat_tagged_pos_doc_list[j]['tag'])\n",
    "    clusters_list.append(cluster_no)\n",
    "import csv\n",
    "\n",
    "with open(\"/home/vaidehi/FlaskApp/cluster/sports.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pymysql.connect(\"localhost\",\"root\",\"root\",\"news_articles\",charset=\"utf8\")\n",
    "cursor = db.cursor()\n",
    "sql = \"SELECT * from articles where genre_id=7\"\n",
    "cursor.execute(sql)\n",
    "articles=cursor.fetchall()\n",
    "news=[]\n",
    "tagged_pos_doc_list=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for article in articles:\n",
    "    article_dict={'ArticleId': article[0],'Headline':article[1], 'Date':article[3],'Summary':article[7],'Image':article[4] ,'GenreId': article[5],'Content':article[2],'Excited':article[8],'Happy':article[9],'Neutral':article[10],'Sad':article[11],'Morose':article[12]}\n",
    "    news.append(article_dict)\n",
    "for i in range(len(news)):\n",
    "    tagged_sent = pos_tag(news[i]['Content'].split())\n",
    "    tagged_sent_proper_nouns=[]\n",
    "    for x in range(len(tagged_sent)):\n",
    "        if tagged_sent[x][1]=='NNP':\n",
    "            tagged_sent_proper_nouns.append(tagged_sent[x][0])\n",
    "        tagged_dict={'tag':news[i]['ArticleId'],'propernoun':tagged_sent_proper_nouns}\n",
    "    tagged_pos_doc_list.append(tagged_dict)\n",
    "y=len(tagged_pos_doc_list)\n",
    "data_set=[]\n",
    "concat_tagged_pos_doc_list=[]\n",
    "for items in tagged_pos_doc_list:\n",
    "    ax=' '.join(items['propernoun'])\n",
    "    str_list=[]\n",
    "    str_list.append(ax)\n",
    "    concat_dict={'tag':items['tag'],'concatstr':str_list}\n",
    "    concat_tagged_pos_doc_list.append(concat_dict)\n",
    "concatstr_list=[]\n",
    "for items in concat_tagged_pos_doc_list:\n",
    "    concatstr_list.append(items['concatstr'][0])\n",
    "\n",
    "data_set=''.join(concatstr_list)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(concatstr_list)\n",
    "X_train_counts.toarray()\n",
    "similarity_graph = X_train_counts * X_train_counts.T #sentence\n",
    "similarity_graph.toarray()\n",
    "z=(int)(y *0.3)\n",
    "spectral = cluster.SpectralClustering(n_clusters=z,eigen_solver='arpack',affinity=\"precomputed\")\n",
    "spectral.fit(similarity_graph)\n",
    "clusters = spectral.fit_predict(similarity_graph)\n",
    "clusters_list=[]\n",
    "for i in range(z):\n",
    "    cluster_no=[]\n",
    "    for j in range(len(clusters)):\n",
    "        if clusters[j]==i:\n",
    "            cluster_no.append(concat_tagged_pos_doc_list[j]['tag'])\n",
    "    clusters_list.append(cluster_no)\n",
    "import csv\n",
    "\n",
    "with open(\"/home/vaidehi/FlaskApp/cluster/entertainment.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
